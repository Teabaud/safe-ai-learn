---
title: Understanding AI Alignment and Safety
lessonId: lesson-2
language: en
---

# [s1] Key Concepts

- AI Alignment (Outer vs Inner)
- Convergent Instrumental Goals
- AI Safety Frameworks
- Agents and Optimizers
- Sycophants vs Schemers
- Transformative AI Risks
- Governance and Control

# [s2] Summary

AI alignment is the challenge of ensuring artificial intelligence systems behave in ways that are beneficial and aligned with human values and intentions. As AI systems become more powerful and autonomous, the complexity of ensuring they do what we want while avoiding unintended consequences grows significantly.

This lesson explores the fundamental concepts of AI alignment, different types of alignment failures, and the broader landscape of AI safety. We'll examine both theoretical frameworks and practical challenges in creating AI systems that reliably pursue intended goals while avoiding potential catastrophic outcomes.

# [s3] Resources

- [r1] [What is AI Alignment?](https://aisafetyfundamentals.com/blog/what-is-ai-alignment/)
  Essential introduction to AI alignment concepts and the distinction between outer and inner alignment (15 min read)

- [r2] [Intro to AI Safety](https://www.youtube.com/watch?v=pYXy-A4siMw&t=16)
  Overview of key AI safety concepts including agents, intelligence, and convergent instrumental goals (20 min video)

- [r3] [How We Could Stumble into AI Catastrophe](https://www.cold-takes.com/how-we-could-stumble-into-ai-catastrophe/)
  Analysis of potential future scenarios with transformative AI development (36 min read)

- [r4] [Why alignment could be hard with modern deep learning](https://www.cold-takes.com/why-ai-alignment-could-be-hard-with-modern-deep-learning/)
  Modern framework for understanding different types of alignment challenges (25 min read)

# [s4] Checkpoints

- [c1] What is the difference between outer and inner alignment? Can you provide an example of each?
- [c2] How do convergent instrumental goals emerge in AI systems, and why might they be concerning?
- [c3] What distinguishes sycophantic behavior from scheming behavior in AI systems?
- [c4] What are the main challenges in ensuring AI systems remain aligned with human values as they become more powerful?
- [c5] How do different stakeholders' values and intentions complicate the alignment problem?