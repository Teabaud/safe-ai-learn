---
title: Comprendre l'alignement et la sûreté de l'IA
lessonId: lesson-2
language: fr
---

# [s1] Concepts clés

- Alignement de l'IA (externe vs interne)
- Objectifs instrumentaux convergents
- Cadres de sûreté de l'IA
- Agents et optimiseurs
- Sycophantes vs stratèges
- Risques liés à l'IA transformative
- Gouvernance et contrôle

# [s2] Résumé

L'alignement de l'IA représente le défi d'assurer que les systèmes d'intelligence artificielle se comportent de manière bénéfique et en accord avec les valeurs et intentions humaines. À mesure que les systèmes d'IA deviennent plus puissants et autonomes, la complexité de s'assurer qu'ils font ce que nous souhaitons tout en évitant les conséquences imprévues augmente considérablement.

Cette leçon explore les concepts fondamentaux de l'alignement de l'IA, les différents types d'échecs d'alignement, et le paysage plus large de la sûreté de l'IA. Nous examinerons à la fois les cadres théoriques et les défis pratiques dans la création de systèmes d'IA qui poursuivent de manière fiable les objectifs prévus tout en évitant les résultats potentiellement catastrophiques.

# [s3] Ressources

- [r1] [Qu'est-ce que l'alignement de l'IA ?](https://aisafetyfundamentals.com/blog/what-is-ai-alignment/)
  Introduction essentielle aux concepts d'alignement de l'IA et à la distinction entre alignement externe et interne (15 min de lecture)

- [r2] [Introduction à la sûreté de l'IA](https://www.youtube.com/watch?v=pYXy-A4siMw&t=16)
  Vue d'ensemble des concepts clés de la sûreté de l'IA, notamment les agents, l'intelligence et les objectifs instrumentaux convergents (20 min de vidéo)

- [r3] [Comment nous pourrions trébucher vers une catastrophe liée à l'IA](https://www.cold-takes.com/how-we-could-stumble-into-ai-catastrophe/)
  Analyse des scénarios futurs potentiels avec le développement de l'IA transformative (36 min de lecture)

- [r4] [Pourquoi l'alignement pourrait être difficile avec l'apprentissage profond moderne](https://www.cold-takes.com/why-ai-alignment-could-be-hard-with-modern-deep-learning/)
  Cadre moderne pour comprendre les différents types de défis d'alignement (25 min de lecture)

# [s4] Points de contrôle

- [c1] Quelle est la différence entre l'alignement externe et interne ? Pouvez-vous donner un exemple de chacun ?
- [c2] Comment les objectifs instrumentaux convergents émergent-ils dans les systèmes d'IA, et pourquoi pourraient-ils être préoccupants ?
- [c3] Qu'est-ce qui distingue le comportement sycophante du comportement stratège dans les systèmes d'IA ?
- [c4] Quels sont les principaux défis pour garantir que les systèmes d'IA restent alignés avec les valeurs humaines à mesure qu'ils deviennent plus puissants ?
- [c5] Comment les valeurs et intentions des différentes parties prenantes compliquent-elles le problème de l'alignement ?
