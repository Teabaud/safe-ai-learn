---
title: Понимание Выравнивания и Безопасности ИИ
lessonId: lesson-2
language: ru
---

# [s1] Ключевые концепции

- Выравнивание ИИ (внешнее и внутреннее)
- Конвергентные инструментальные цели
- Основы безопасности ИИ
- Агенты и оптимизаторы
- Угодники против интриганов
- Риски трансформационного ИИ
- Управление и контроль

# [s2] Краткое содержание

Выравнивание ИИ — это задача обеспечения того, чтобы системы искусственного интеллекта действовали способами, которые приносят пользу и соответствуют человеческим ценностям и намерениям. По мере того как системы ИИ становятся более мощными и автономными, существенно возрастает сложность обеспечения того, чтобы они делали то, что мы хотим, избегая при этом непредвиденных последствий.

В этом уроке рассматриваются фундаментальные концепции выравнивания ИИ, различные типы сбоев выравнивания и более широкая картина безопасности ИИ. Мы изучим как теоретические основы, так и практические проблемы создания систем ИИ, которые надёжно преследуют намеченные цели, избегая при этом потенциальных катастрофических результатов.

# [s3] Источники

- [r1] [Что такое выравнивание ИИ?](https://aisafetyfundamentals.com/blog/what-is-ai-alignment/)
  Базовое введение в концепции выравнивания ИИ и различие между внешним и внутренним выравниванием (15 минут чтения)

- [r2] [Введение в безопасность ИИ](https://www.youtube.com/watch?v=pYXy-A4siMw&t=16)
  Обзор ключевых концепций безопасности ИИ, включая агентов, интеллект и конвергентные инструментальные цели (20 минут видео)

- [r3] [Как мы можем случайно прийти к катастрофе ИИ](https://www.cold-takes.com/how-we-could-stumble-into-ai-catastrophe/)
  Анализ возможных будущих сценариев развития трансформационного ИИ (36 минут чтения)

- [r4] [Почему выравнивание может быть сложным при современном глубоком обучении](https://www.cold-takes.com/why-ai-alignment-could-be-hard-with-modern-deep-learning/)
  Современный подход к пониманию различных типов проблем выравнивания (25 минут чтения)

# [s4] Контрольные вопросы

- [c1] В чём разница между внешним и внутренним выравниванием? Можете привести пример каждого?
- [c2] Как возникают конвергентные инструментальные цели в системах ИИ, и почему они могут вызывать беспокойство?
- [c3] Чем отличается угодническое поведение от интриганского в системах ИИ?
- [c4] Каковы основные проблемы в обеспечении того, чтобы системы ИИ оставались согласованными с человеческими ценностями по мере того, как они становятся более мощными?
- [c5] Как различные ценности и намерения заинтересованных сторон усложняют проблему выравнивания?
